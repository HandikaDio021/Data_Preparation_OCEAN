{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Fullscene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update annotation path !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:05<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Ekstraksi selesai untuk train. Frame disimpan di: dataset\\fullscene\\train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset train disimpan dalam format tf.data di: data\\fullscene\\train_ds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Ekstraksi selesai untuk val. Frame disimpan di: dataset\\fullscene\\val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset val disimpan dalam format tf.data di: data\\fullscene\\val_ds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Ekstraksi selesai untuk test. Frame disimpan di: dataset\\fullscene\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset test disimpan dalam format tf.data di: data\\fullscene\\test_ds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from utils import extract_fullscene, load_images, load_annotations\n",
    "\n",
    "# List subset yang akan diproses\n",
    "subsets = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "# Memuat anotasi\n",
    "annotation_train, annotation_valid, annotation_test = load_annotations()\n",
    "annotations = {\"train\": annotation_train, \"val\": annotation_valid, \"test\": annotation_test}\n",
    "\n",
    "for subset in subsets:\n",
    "    video_dir = os.path.join(\"dataset\", \"videos\", subset)\n",
    "    save_dir = os.path.join(\"dataset\", \"fullscene\", subset)\n",
    "    \n",
    "    # Membuat folder output jika belum ada\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Ekstraksi frame dari video\n",
    "    extract_fullscene(video_dir, save_dir, num_images=10, image_size=(224, 224))\n",
    "    \n",
    "    print(f\"[INFO] Ekstraksi selesai untuk {subset}. Frame disimpan di: {save_dir}\")\n",
    "    \n",
    "    # Memuat anotasi sesuai subset\n",
    "    annotation_data = annotations[subset]\n",
    "    \n",
    "    # Memuat gambar ke dalam tf.data.Dataset\n",
    "    dataset = load_images(save_dir, annotation_data)\n",
    "    \n",
    "    # Menyimpan dataset dalam format tf.data\n",
    "    save_ds_dir = os.path.join(\"data\", \"fullscene\", f\"{subset}_ds\")\n",
    "    os.makedirs(save_ds_dir, exist_ok=True)  # Buat folder jika belum ada\n",
    "    dataset.save(save_ds_dir)\n",
    "    print(f\"[INFO] Dataset {subset} disimpan dalam format tf.data di: {save_ds_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update annotation path !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:17<00:00,  5.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Ekstraksi selesai untuk train. Frame disimpan di: dataset\\faces\\train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset train disimpan dalam format tf.data di: data\\faces\\train_ds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Ekstraksi selesai untuk val. Frame disimpan di: dataset\\faces\\val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset val disimpan dalam format tf.data di: data\\faces\\val_ds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Ekstraksi selesai untuk test. Frame disimpan di: dataset\\faces\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset test disimpan dalam format tf.data di: data\\faces\\test_ds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from utils import extract_face, load_images, load_annotations\n",
    "\n",
    "# List subset yang akan diproses\n",
    "subsets = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "# Memuat anotasi\n",
    "annotation_train, annotation_valid, annotation_test = load_annotations()\n",
    "annotations = {\"train\": annotation_train, \"val\": annotation_valid, \"test\": annotation_test}\n",
    "\n",
    "for subset in subsets:\n",
    "    image_dir = os.path.join(\"dataset\", \"fullscene\", subset)\n",
    "    save_dir = os.path.join(\"dataset\", \"faces\", subset)\n",
    "    \n",
    "    # Membuat folder output jika belum ada\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Ekstraksi frame dari video\n",
    "    extract_face(image_dir, save_dir, image_size=(224, 224))\n",
    "    \n",
    "    print(f\"[INFO] Ekstraksi selesai untuk {subset}. Frame disimpan di: {save_dir}\")\n",
    "    \n",
    "    # Memuat anotasi sesuai subset\n",
    "    annotation_data = annotations[subset]\n",
    "    \n",
    "    # Memuat gambar ke dalam tf.data.Dataset\n",
    "    dataset = load_images(save_dir, annotation_data)\n",
    "    \n",
    "    # Menyimpan dataset dalam format tf.data\n",
    "    save_ds_dir = os.path.join(\"data\", \"faces\", f\"{subset}_ds\")\n",
    "    os.makedirs(save_ds_dir, exist_ok=True)  # Buat folder jika belum ada\n",
    "    dataset.save(save_ds_dir)\n",
    "    print(f\"[INFO] Dataset {subset} disimpan dalam format tf.data di: {save_ds_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Memuat anotasi dan transkripsi...\n",
      "Update annotation path !\n",
      "Update transcriptions path !\n",
      "[INFO] Teks dari train diproses.\n",
      "[INFO] Teks dari val diproses.\n",
      "[INFO] Teks dari test diproses.\n",
      "[INFO] Mengunduh GloVe embeddings (100D), mohon tunggu...\n",
      "Total kata unik dalam dataset: 88\n",
      "Vocab Size yang digunakan: 89\n",
      "[INFO] Embedding matrix disimpan dengan shape (89, 100)\n",
      "[INFO] Dataset teks dalam format `tf.data.Dataset` selesai disimpan!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from utils import process_text, preprocess_text, load_annotations, load_transcriptions\n",
    "\n",
    "# Load Anotasi & Transkripsi\n",
    "print(\"[INFO] Memuat anotasi dan transkripsi...\")\n",
    "annotation_train, annotation_valid, annotation_test = load_annotations()\n",
    "transcr_train, transcr_valid, transcr_test = load_transcriptions()\n",
    "\n",
    "annotations = {\"train\": annotation_train, \"val\": annotation_valid, \"test\": annotation_test}\n",
    "transcriptions = {\"train\": transcr_train, \"val\": transcr_valid, \"test\": transcr_test}\n",
    "\n",
    "# Memproses Teks dari Transkripsi\n",
    "subsets = [\"train\", \"val\", \"test\"]\n",
    "all_texts = []  # Untuk tokenizer\n",
    "datasets = {}  # Menyimpan dataset per subset\n",
    "\n",
    "for subset in subsets:\n",
    "    video_dir = os.path.join(\"dataset\", \"videos\", subset)\n",
    "    save_text_dir = os.path.join(\"dataset\", \"text\", subset)\n",
    "    os.makedirs(save_text_dir, exist_ok=True)  \n",
    "\n",
    "    annotation_data = annotations[subset]\n",
    "    transcription_data = transcriptions[subset]\n",
    "\n",
    "    # Proses teks menggunakan `process_text`\n",
    "    text_df = process_text(video_dir, annotation_data, transcription_data)\n",
    "\n",
    "    # Bersihkan teks menggunakan `preprocess_text`\n",
    "    text_df[\"text\"] = text_df[\"text\"].apply(preprocess_text)\n",
    "\n",
    "    # Simpan teks untuk tokenizer\n",
    "    all_texts.extend(text_df[\"text\"].tolist())\n",
    "\n",
    "    # Simpan dataset sementara\n",
    "    datasets[subset] = text_df\n",
    "\n",
    "    print(f\"[INFO] Teks dari {subset} diproses.\")\n",
    "\n",
    "# Membuat Tokenizer & Embedding GloVe\n",
    "print(\"[INFO] Mengunduh GloVe embeddings (100D), mohon tunggu...\")\n",
    "glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# Buat tokenizer dari teks yang telah diproses\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")  \n",
    "tokenizer.fit_on_texts(all_texts)  \n",
    "\n",
    "# Sesuaikan vocab_size dengan jumlah kata unik dalam dataset\n",
    "vocab_size = len(tokenizer.word_index) + 1  \n",
    "embed_size = 100  \n",
    "\n",
    "print(f\"Total kata unik dalam dataset: {len(tokenizer.word_index)}\")\n",
    "print(f\"Vocab Size yang digunakan: {vocab_size}\")\n",
    "\n",
    "# Buat embedding matrix\n",
    "embed_matrix = np.zeros((vocab_size, embed_size))\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    if idx < vocab_size:\n",
    "        embedding_vector = glove_model[word] if word in glove_model else None\n",
    "        if embedding_vector is not None:\n",
    "            embed_matrix[idx] = embedding_vector  \n",
    "\n",
    "# Simpan embedding matrix\n",
    "embed_dir = \"data/text/\"\n",
    "os.makedirs(embed_dir, exist_ok=True)\n",
    "np.save(os.path.join(embed_dir, \"embed_matrix.npy\"), embed_matrix)\n",
    "\n",
    "print(f\"[INFO] Embedding matrix disimpan dengan shape {embed_matrix.shape}\")\n",
    "\n",
    "# Menyimpan Dataset dalam Format `tf.data.Dataset` (Dalam Bentuk Token)\n",
    "save_tf_dataset_dir = \"data/text/\"\n",
    "os.makedirs(save_tf_dataset_dir, exist_ok=True)\n",
    "\n",
    "def tokenize_and_pad(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(sequences, maxlen=50)  # Sesuaikan dengan sentlen\n",
    "\n",
    "for subset in subsets:\n",
    "    text_df = datasets[subset]\n",
    "\n",
    "    # Ubah teks menjadi tokenized sequences\n",
    "    tokenized_texts = tokenize_and_pad(text_df[\"text\"])\n",
    "\n",
    "    # Simpan dalam format tf.data.Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((tokenized_texts, text_df[['o', 'c', 'e', 'a', 'n']].values))\n",
    "    dataset.save(os.path.join(save_tf_dataset_dir, f\"{subset}_ds\"))\n",
    "\n",
    "print(\"[INFO] Dataset teks dalam format `tf.data.Dataset` selesai disimpan!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handika",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
